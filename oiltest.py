{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9975,"status":"ok","timestamp":1631013099659,"user":{"displayName":"Alexander Lee","photoUrl":"","userId":"12627238482128920563"},"user_tz":-180},"id":"vaAqaBmo8V8A","outputId":"515d53f8-ec94-4bc0-f34e-1a9100b3a5b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting yahoo_fin\n","  Downloading yahoo_fin-0.8.9.1-py3-none-any.whl (10 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from yahoo_fin) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from yahoo_fin) (1.1.5)\n","Collecting requests-html\n","  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n","Collecting feedparser\n","  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n","\u001b[K     |████████████████████████████████| 81 kB 4.2 MB/s \n","\u001b[?25hCollecting sgmllib3k\n","  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n","Requirement already satisfied: pytz\u003e=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas-\u003eyahoo_fin) (2018.9)\n","Requirement already satisfied: numpy\u003e=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas-\u003eyahoo_fin) (1.19.5)\n","Requirement already satisfied: python-dateutil\u003e=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas-\u003eyahoo_fin) (2.8.2)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil\u003e=2.7.3-\u003epandas-\u003eyahoo_fin) (1.15.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003eyahoo_fin) (1.24.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003eyahoo_fin) (2021.5.30)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003eyahoo_fin) (3.0.4)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003eyahoo_fin) (2.10)\n","Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (from requests-html-\u003eyahoo_fin) (0.0.1)\n","Collecting parse\n","  Downloading parse-1.19.0.tar.gz (30 kB)\n","Collecting pyquery\n","  Downloading pyquery-1.4.3-py3-none-any.whl (22 kB)\n","Collecting fake-useragent\n","  Downloading fake-useragent-0.1.11.tar.gz (13 kB)\n","Collecting w3lib\n","  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n","Collecting pyppeteer\u003e=0.0.14\n","  Downloading pyppeteer-0.2.6-py3-none-any.whl (83 kB)\n","\u001b[K     |████████████████████████████████| 83 kB 2.3 MB/s \n","\u001b[?25hCollecting pyee\u003c9.0.0,\u003e=8.1.0\n","  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n","Collecting urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 40.6 MB/s \n","\u001b[?25hCollecting websockets\u003c10.0,\u003e=9.1\n","  Downloading websockets-9.1-cp37-cp37m-manylinux2010_x86_64.whl (103 kB)\n","\u001b[K     |████████████████████████████████| 103 kB 49.5 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm\u003c5.0.0,\u003e=4.42.1 in /usr/local/lib/python3.7/dist-packages (from pyppeteer\u003e=0.0.14-\u003erequests-html-\u003eyahoo_fin) (4.62.0)\n","Requirement already satisfied: importlib-metadata\u003e=1.4 in /usr/local/lib/python3.7/dist-packages (from pyppeteer\u003e=0.0.14-\u003erequests-html-\u003eyahoo_fin) (4.6.4)\n","Requirement already satisfied: appdirs\u003c2.0.0,\u003e=1.4.3 in /usr/local/lib/python3.7/dist-packages (from pyppeteer\u003e=0.0.14-\u003erequests-html-\u003eyahoo_fin) (1.4.4)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=1.4-\u003epyppeteer\u003e=0.0.14-\u003erequests-html-\u003eyahoo_fin) (3.5.0)\n","Requirement already satisfied: typing-extensions\u003e=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=1.4-\u003epyppeteer\u003e=0.0.14-\u003erequests-html-\u003eyahoo_fin) (3.7.4.3)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4-\u003erequests-html-\u003eyahoo_fin) (4.6.3)\n","Requirement already satisfied: lxml\u003e=2.1 in /usr/local/lib/python3.7/dist-packages (from pyquery-\u003erequests-html-\u003eyahoo_fin) (4.2.6)\n","Collecting cssselect\u003e0.7.9\n","  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n","Building wheels for collected packages: fake-useragent, parse, sgmllib3k\n","  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-py3-none-any.whl size=13502 sha256=2adc43e867d7c29de93dc022e1befe0e60e4ea864fbf91d26a13f6b7bec98e71\n","  Stored in directory: /root/.cache/pip/wheels/ed/f7/62/50ab6c9a0b5567267ab76a9daa9d06315704209b2c5d032031\n","  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for parse: filename=parse-1.19.0-py3-none-any.whl size=24591 sha256=5d7646cacb193f85fbbe2a011c761059dd38f267b7641f9a13575fd8e650552d\n","  Stored in directory: /root/.cache/pip/wheels/9c/aa/cc/f2228050ccb40f22144b073f15a2c84f11204f29fc0dce028e\n","  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=04f05bf742a7b9aae4cf0d0ceb00cb4e4afa09c9771b232015036f94defeb5a2\n","  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n","Successfully built fake-useragent parse sgmllib3k\n","Installing collected packages: websockets, urllib3, pyee, cssselect, w3lib, sgmllib3k, pyquery, pyppeteer, parse, fake-useragent, requests-html, feedparser, yahoo-fin\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed cssselect-1.1.0 fake-useragent-0.1.11 feedparser-6.0.8 parse-1.19.0 pyee-8.2.2 pyppeteer-0.2.6 pyquery-1.4.3 requests-html-0.10.0 sgmllib3k-1.0.0 urllib3-1.25.11 w3lib-1.22.0 websockets-9.1 yahoo-fin-0.8.9.1\n"]}],"source":["!pip install yahoo_fin"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"ANAgxCOi8Npi"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/60\n","64/64 [==============================] - 243s 4s/step - loss: 0.0156 - mean_absolute_error: 0.1078 - val_loss: 4.4936e-04 - val_mean_absolute_error: 0.0211\n","\n","Epoch 00001: val_loss improved from inf to 0.00045, saving model to /home/ubuntu/Desktop/TelegramBot/tensorflow/tensorflowdata/resultsOIL/2021-09-07_CL=F-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-7-layers-3-units-512.h5\n"]},{"ename":"OSError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-4-1217dc13e699\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"X_test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 220\u001b[0;31m                     verbose=1)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1228\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1230\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 413\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1366\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1368\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_should_save_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[0;34m(self, epoch, batch, logs)\u001b[0m\n\u001b[1;32m   1446\u001b[0m                         'directory: {}'.format(filepath))\n\u001b[1;32m   1447\u001b[0m         \u001b[0;31m# Re-throw the error for any other causes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1448\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_file_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[0;34m(self, epoch, batch, logs)\u001b[0m\n\u001b[1;32m   1418\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m                 self.model.save_weights(\n\u001b[0;32m-\u003e 1420\u001b[0;31m                     filepath, overwrite=True, options=self._options)\n\u001b[0m\u001b[1;32m   1421\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite, save_format, options)\u001b[0m\n\u001b[1;32m   2249\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'h5'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 2251\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2252\u001b[0m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2253\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, **kwds)\u001b[0m\n\u001b[1;32m    425\u001b[0m                                fapl, fcpl=make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001b[1;32m    426\u001b[0m                                fs_persist=fs_persist, fs_threshold=fs_threshold),\n\u001b[0;32m--\u003e 427\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 196\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = '/home/ubuntu/Desktop/TelegramBot/tensorflow/tensorflowdata/resultsOIL/2021-09-07_CL=F-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-7-layers-3-units-512.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n","from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from yahoo_fin import stock_info as si\n","from collections import deque\n","import matplotlib.pyplot as plt\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","import random\n","\n","import os\n","import time\n","from tensorflow.keras.layers import LSTM\n","\n","\n","np.random.seed(314)\n","tf.random.set_seed(314)\n","random.seed(314)\n","\n","def shuffle_in_unison(a, b):\n","    # shuffle two arrays in the same way\n","    state = np.random.get_state()\n","    np.random.shuffle(a)\n","    np.random.set_state(state)\n","    np.random.shuffle(b)\n","\n","def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True,\n","                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n","\n","    # see if ticker is already a loaded stock from yahoo finance\n","    if isinstance(ticker, str):\n","        # load it from yahoo_fin library\n","        df = si.get_data(ticker)\n","    elif isinstance(ticker, pd.DataFrame):\n","        # already loaded, use it directly\n","        df = si.get_data(ticker)\n","    else:\n","        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n","    # this will contain all the elements we want to return from this function\n","    result = {}\n","    # we will also return the original dataframe itself\n","    result['df'] = df.copy()\n","    # make sure that the passed feature_columns exist in the dataframe\n","    for col in feature_columns:\n","        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n","    # add date as a column\n","    if \"date\" not in df.columns:\n","        df[\"date\"] = df.index\n","    if scale:\n","        column_scaler = {}\n","        # scale the data (prices) from 0 to 1\n","        for column in feature_columns:\n","            scaler = preprocessing.MinMaxScaler()\n","            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n","            column_scaler[column] = scaler\n","        # add the MinMaxScaler instances to the result returned\n","        result[\"column_scaler\"] = column_scaler\n","    # add the target column (label) by shifting by `lookup_step`\n","    df['future'] = df['adjclose'].shift(-lookup_step)\n","    # last `lookup_step` columns contains NaN in future column\n","    # get them before droping NaNs\n","    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n","    # drop NaNs\n","    df.dropna(inplace=True)\n","    sequence_data = []\n","    sequences = deque(maxlen=n_steps)\n","    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n","        sequences.append(entry)\n","        if len(sequences) == n_steps:\n","            sequence_data.append([np.array(sequences), target])\n","    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n","    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\n","    # this last_sequence will be used to predict future stock prices that are not available in the dataset\n","    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n","    last_sequence = np.array(last_sequence).astype(np.float32)\n","    # add to result\n","    result['last_sequence'] = last_sequence\n","    # construct the X's and y's\n","    X, y = [], []\n","    for seq, target in sequence_data:\n","        X.append(seq)\n","        y.append(target)\n","    # convert to numpy arrays\n","    X = np.array(X)\n","    y = np.array(y)\n","    if split_by_date:\n","        # split the dataset into training \u0026 testing sets by date (not randomly splitting)\n","        train_samples = int((1 - test_size) * len(X))\n","        result[\"X_train\"] = X[:train_samples]\n","        result[\"y_train\"] = y[:train_samples]\n","        result[\"X_test\"]  = X[train_samples:]\n","        result[\"y_test\"]  = y[train_samples:]\n","        if shuffle:\n","            # shuffle the datasets for training (if shuffle parameter is set)\n","            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n","            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n","    else:\n","        # split the dataset randomly\n","        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y,\n","                                                                                test_size=test_size, shuffle=shuffle)\n","    # get the list of test set dates\n","    dates = result[\"X_test\"][:, -1, -1]\n","    # retrieve test features from the original dataframe\n","    result[\"test_df\"] = result[\"df\"].loc[dates]\n","    # remove duplicated dates in the testing dataframe\n","    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n","    # remove dates from the training/testing sets \u0026 convert to float32\n","    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n","    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n","    return result\n","\n","def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n","                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n","    model = Sequential()\n","    for i in range(n_layers):\n","        if i == 0:\n","            # first layer\n","            if bidirectional:\n","                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n","            else:\n","                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n","        elif i == n_layers - 1:\n","            # last layer\n","            if bidirectional:\n","                model.add(Bidirectional(cell(units, return_sequences=False)))\n","            else:\n","                model.add(cell(units, return_sequences=False))\n","        else:\n","            # hidden layers\n","            if bidirectional:\n","                model.add(Bidirectional(cell(units, return_sequences=True)))\n","            else:\n","                model.add(cell(units, return_sequences=True))\n","        # add dropout after each layer\n","        model.add(Dropout(dropout))\n","    model.add(Dense(1, activation=\"linear\"))\n","    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n","    return model\n","\n","\n","# Window size or the sequence length\n","N_STEPS = 50\n","# Lookup step, 1 is the next day\n","LOOKUP_STEP = 7\n","# whether to scale feature columns \u0026 output price as well\n","SCALE = True\n","scale_str = f\"sc-{int(SCALE)}\"\n","# whether to shuffle the dataset\n","SHUFFLE = True\n","shuffle_str = f\"sh-{int(SHUFFLE)}\"\n","# whether to split the training/testing set by date\n","SPLIT_BY_DATE = False\n","split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n","# test ratio size, 0.2 is 20%\n","TEST_SIZE = 0.2\n","# features to use\n","FEATURE_COLUMNS = [\"adjclose\", 'volume', 'open', 'high', 'low']\n","# date now\n","date_now = time.strftime(\"%Y-%m-%d\")\n","### model parameters\n","N_LAYERS = 3\n","# LSTM cell\n","CELL = LSTM\n","# 256 LSTM neurons\n","UNITS = 512\n","# 40% dropout\n","DROPOUT = 0.5\n","# whether to use bidirectional RNNs\n","BIDIRECTIONAL = False\n","### training parameters\n","# mean absolute error loss\n","# LOSS = \"mae\"\n","# huber loss\n","LOSS = \"huber_loss\"\n","OPTIMIZER = \"adam\"\n","BATCH_SIZE = 64\n","EPOCHS = 60\n","# Amazon stock market\n","ticker = \"CL=F\"\n","ticker_data_filename = os.path.join(\"dataOIL\", f\"{ticker}_{date_now}.csv\")\n","# model name to save, making it as unique as possible based on parameters\n","model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n","{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n","if BIDIRECTIONAL:\n","    model_name += \"-b\"\n","\n","\n","# create these folders if they does not exist\n","if not os.path.isdir(\"resultsOIL\"):\n","    os.mkdir(\"resultsOIL\")\n","if not os.path.isdir(\"logsOIL\"):\n","    os.mkdir(\"logsOIL\")\n","if not os.path.isdir(\"dataOIL\"):\n","    os.mkdir(\"dataOIL\")\n","\n","\n","# load the data\n","data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE,\n","                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE,\n","                feature_columns=FEATURE_COLUMNS)\n","# save the dataframe\n","#data[\"df\"].to_csv(ticker_data_filename)\n","# construct the model\n","model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n","                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n","# some tensorflow callbacks\n","checkpointer = ModelCheckpoint(os.path.join(\"resultsOIL\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n","tensorboard = TensorBoard(log_dir=os.path.join(\"logsOIL\", model_name))\n","# train the model and save the weights whenever we see\n","# a new optimal model using ModelCheckpoint\n","history = model.fit(data[\"X_train\"], data[\"y_train\"],\n","                    batch_size=BATCH_SIZE,\n","                    epochs=EPOCHS,\n","                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n","                    callbacks=[checkpointer, tensorboard],\n","                    verbose=1)\n","\n","\n","def plot_graph(test_df):\n","    \"\"\"\n","    This function plots true close price along with predicted close price\n","    with blue and red colors respectively\n","    \"\"\"\n","    fig = plt.figure()\n","    plt.plot(test_df[f'true_adjclose_{LOOKUP_STEP}'], c='b')\n","    plt.plot(test_df[f'adjclose_{LOOKUP_STEP}'], c='r')\n","    plt.xlabel(\"Days\")\n","    plt.ylabel(\"Price\")\n","    plt.legend([\"Actual Price\", \"Predicted Price\"])\n","    plt.show()\n","    fig.savefig('OILml.jpeg', dpi=400, bbox_inches='tight')\n","\n","\n","def get_final_df(model, data):\n","    \"\"\"\n","    This function takes the `model` and `data` dict to\n","    construct a final dataframe that includes the features along\n","    with true and predicted prices of the testing dataset\n","    \"\"\"\n","    # if predicted future price is higher than the current,\n","    # then calculate the true future price minus the current price, to get the buy profit\n","    buy_profit  = lambda current, pred_future, true_future: true_future - current if pred_future \u003e current else 0\n","    # if the predicted future price is lower than the current price,\n","    # then subtract the true future price from the current price\n","    sell_profit = lambda current, pred_future, true_future: current - true_future if pred_future \u003c current else 0\n","    X_test = data[\"X_test\"]\n","    y_test = data[\"y_test\"]\n","    # perform prediction and get prices\n","    y_pred = model.predict(X_test)\n","    if SCALE:\n","        y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n","        y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n","    test_df = data[\"test_df\"]\n","    # add predicted future prices to the dataframe\n","    test_df[f\"adjclose_{LOOKUP_STEP}\"] = y_pred\n","    # add true future prices to the dataframe\n","    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = y_test\n","    # sort the dataframe by date\n","    test_df.sort_index(inplace=True)\n","    final_df = test_df\n","    # add the buy profit column\n","    final_df[\"buy_profit\"] = list(map(buy_profit,\n","                                    final_df[\"adjclose\"],\n","                                    final_df[f\"adjclose_{LOOKUP_STEP}\"],\n","                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n","                                    # since we don't have profit for last sequence, add 0's\n","                                    )\n","    # add the sell profit column\n","    final_df[\"sell_profit\"] = list(map(sell_profit,\n","                                    final_df[\"adjclose\"],\n","                                    final_df[f\"adjclose_{LOOKUP_STEP}\"],\n","                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n","                                    # since we don't have profit for last sequence, add 0's\n","                                    )\n","    return final_df\n","\n","\n","def predict(model, data):\n","    # retrieve the last sequence from data\n","    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n","    # expand dimension\n","    last_sequence = np.expand_dims(last_sequence, axis=0)\n","    # get the prediction (scaled from 0 to 1)\n","    prediction = model.predict(last_sequence)\n","    # get the price (by inverting the scaling)\n","    if SCALE:\n","        predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n","    else:\n","        predicted_price = prediction[0][0]\n","    return predicted_price\n","\n","# load optimal model weights from results folder\n","model_path = os.path.join(\"resultsOIL\", model_name) + \".h5\"\n","model.load_weights(model_path)\n","\n","# evaluate the model\n","loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n","# calculate the mean absolute error (inverse scaling)\n","if SCALE:\n","    mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n","else:\n","    mean_absolute_error = mae\n","\n","    # get the final dataframe for the testing set\n","final_df = get_final_df(model, data)\n","\n","# predict the future price\n","future_price = predict(model, data)\n","\n","# we calculate the accuracy by counting the number of positive profits\n","accuracy_score = (len(final_df[final_df['sell_profit'] \u003e 0]) + len(final_df[final_df['buy_profit'] \u003e 0])) / len(final_df)\n","# calculating total buy \u0026 sell profit\n","total_buy_profit  = final_df[\"buy_profit\"].sum()\n","total_sell_profit = final_df[\"sell_profit\"].sum()\n","# total profit by adding sell \u0026 buy together\n","total_profit = total_buy_profit + total_sell_profit\n","# dividing total profit by number of testing samples (number of trades)\n","profit_per_trade = total_profit / len(final_df)\n","\n","\n","# printing metrics\n","print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n","print(f\"{LOSS} loss:\", loss)\n","print(\"Mean Absolute Error:\", mean_absolute_error)\n","print(\"Accuracy score:\", accuracy_score)\n","print(\"Total buy profit:\", total_buy_profit)\n","print(\"Total sell profit:\", total_sell_profit)\n","print(\"Total profit:\", total_profit)\n","print(\"Profit per trade:\", profit_per_trade)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nEcM55JD8Yvr"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sYwBnSv48Y5I"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMutKMUcpwqUTbdMMVeskjh","name":"oiltest.py","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}